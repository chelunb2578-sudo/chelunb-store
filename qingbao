#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¨èƒæƒ…æŠ¥å…³é”®è¯æå– - å®Œæ•´åˆ†æè„šæœ¬ï¼ˆ2021-2025è¿‘äº”å¹´ï¼‰
æ•°æ®æ¥æºï¼šCVE.org 5.0 æ ¼å¼ JSON
åˆ†æå¯¹è±¡ï¼šWebåº”ç”¨ç›¸å…³æ¼æ´
åˆ†ææ–¹æ³•ï¼šTF-IDF + è·¨å¹´è¶‹åŠ¿åˆ†æ
"""

import os
import json
import re
import warnings
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

# ===== æ–°å¢ï¼šç”¨äºè¶‹åŠ¿åˆ†æçš„åº“ =====
try:
    import seaborn as sns
    HAS_SEABORN = True
except ImportError:
    HAS_SEABORN = False
    print("è­¦å‘Šï¼šseabornæœªå®‰è£…ï¼Œçƒ­åŠ›å›¾å°†è·³è¿‡")

# å¯é€‰ï¼šè¯äº‘ï¼ˆç”¨äºå¯è§†åŒ–å¢å¼ºï¼Œä½†ä¸å¼ºåˆ¶ï¼‰
try:
    from wordcloud import WordCloud
    HAS_WORDCLOUD = True
except ImportError:
    HAS_WORDCLOUD = False

warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 60)
print("å¨èƒæƒ…æŠ¥å…³é”®è¯æå–åˆ†æï¼ˆ2021-2025è¿‘äº”å¹´ï¼‰")
print("=" * 60)

# =============================================================================
# ç¬¬ä¸€æ­¥ï¼šåªè¯»å–2021-2025å¹´ä»½æ–‡ä»¶å¤¹é‡Œçš„JSONï¼Œå¹¶è®°å½•å¹´ä»½
# =============================================================================
print("\nã€æ­¥éª¤1ã€‘è¯»å–2021-2025å¹´ä»½çš„CVEè¯¦æƒ…æ–‡ä»¶...")

base_path = r'C:\Users\Hu Kioskky\Downloads\cvelistV5-main\cves'

target_years = ['2021', '2022', '2023', '2024', '2025']
all_json_files = []

for year in target_years:
    year_path = os.path.join(base_path, year)
    if os.path.exists(year_path) and os.path.isdir(year_path):
        for root, dirs, files in os.walk(year_path):
            for file in files:
                if file.endswith('.json'):
                    all_json_files.append(os.path.join(root, file))
        print(f"  {year}å¹´: æ‰¾åˆ° {len([f for f in all_json_files if f'/{year}/' in f or f'\\{year}\\' in f])} ä¸ªæ–‡ä»¶")
    else:
        print(f"  âš ï¸  {year}å¹´æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè·³è¿‡")

print(f"\nå…±æ‰¾åˆ° {len(all_json_files)} ä¸ªJSONæ–‡ä»¶ï¼ˆ2021-2025ï¼‰")

if len(all_json_files) == 0:
    print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
    exit()

MAX_FILES = 0
if MAX_FILES > 0 and len(all_json_files) > MAX_FILES:
    print(f"ç”±äºæ•°æ®é‡è¾ƒå¤§ï¼Œåªè¯»å–å‰{MAX_FILES}ä¸ªæ–‡ä»¶ï¼ˆå¯ä¿®æ”¹MAX_FILESè°ƒæ•´ï¼‰")
    all_json_files = all_json_files[:MAX_FILES]
else:
    print(f"å‡†å¤‡è¯»å–å…¨éƒ¨ {len(all_json_files)} ä¸ªæ–‡ä»¶...")

raw_data = []
failed_files = []

for i, file_path in enumerate(all_json_files):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            cve_data = json.load(f)

        cve_id = cve_data.get('cveMetadata', {}).get('cveId', 'Unknown')

        # ===== æ–°å¢ï¼šä»CVE IDæå–å¹´ä»½ =====
        if cve_id.startswith('CVE-'):
            year = cve_id.split('-')[1]
        else:
            year = 'unknown'

        description = ""
        try:
            containers = cve_data.get('containers', {})
            cna = containers.get('cna', {})
            desc_list = cna.get('descriptions', [])
            if desc_list:
                for d in desc_list:
                    if d.get('lang') == 'en':
                        description = d.get('value', '')
                        break
                if not description and desc_list:
                    description = desc_list[0].get('value', '')
        except:
            pass

        if not description:
            try:
                desc_list = cve_data.get('descriptions', [])
                if desc_list:
                    description = desc_list[0].get('value', '')
            except:
                pass

        if description and len(description) > 10:
            raw_data.append({
                'Name': cve_id,
                'Description': description,
                'Year': year   # ===== æ–°å¢å¹´ä»½å­—æ®µ =====
            })

        if (i + 1) % 500 == 0:
            print(f"  å·²å¤„ç† {i + 1}/{len(all_json_files)}ï¼ŒæˆåŠŸ {len(raw_data)} æ¡")

    except Exception as e:
        failed_files.append((file_path, str(e)))

print(f"\nâœ… æˆåŠŸè¯»å– {len(raw_data)} æ¡CVEè®°å½•ï¼ˆè¿‘äº”å¹´ï¼‰")
if failed_files:
    print(f"âš ï¸  å¤±è´¥ {len(failed_files)} ä¸ªæ–‡ä»¶")

df = pd.DataFrame(raw_data)
print(f"\nğŸ“Š åŸå§‹æ•°æ®ï¼š{len(df)} æ¡")
print(f"å¹´ä»½åˆ†å¸ƒï¼š\n{df['Year'].value_counts().sort_index()}")

# =============================================================================
# ç¬¬äºŒæ­¥ï¼šç­›é€‰Webç›¸å…³CVE
# =============================================================================
print("\n" + "=" * 60)
print("ã€æ­¥éª¤2ã€‘ç­›é€‰Webç›¸å…³CVE...")

web_keywords = [
    'sql', 'injection', 'xss', 'cross-site', 'scripting', 'csrf',
    'traversal', 'execution', 'rce', 'command', 'bypass', 'escalation',
    'disclosure', 'denial', 'overflow', 'web', 'http', 'https', 'php',
    'javascript', 'apache', 'nginx', 'tomcat', 'wordpress', 'drupal',
    'cms', 'database', 'oracle', 'mysql', 'postgres', 'mongo',
    'authentication', 'privilege', 'credential', 'password', 'session',
    'cookie', 'header', 'request', 'response', 'url', 'uri', 'endpoint',
    'api', 'rest', 'graphql', 'soap', 'xml', 'json', 'html', 'css',
    'upload', 'download', 'file', 'path', 'directory', 'folder'
]

pattern = '|'.join([re.escape(kw) for kw in web_keywords])

df['is_web'] = df['Description'].str.contains(pattern, case=False, na=False)
web_df = df[df['is_web']].copy()

print(f"ğŸ“Š Webç›¸å…³ï¼š{len(web_df)} æ¡ï¼Œå æ¯” {len(web_df) / len(df) * 100:.1f}%")

if len(web_df) == 0:
    print("âš ï¸ æœªæ‰¾åˆ°Webç›¸å…³ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
    web_df = df.copy()

web_df.to_csv('step2_web_cves_2021_2025.csv', index=False)
print("å·²ä¿å­˜è‡³ï¼šstep2_web_cves_2021_2025.csv")

# =============================================================================
# ç¬¬ä¸‰æ­¥ï¼šæ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†ï¼ˆç»ˆæä¼˜åŒ–ç‰ˆ - ä¸“æ³¨æ”»å‡»æŠ€æœ¯ï¼‰
# =============================================================================
print("\n" + "=" * 60)
print("ã€æ­¥éª¤3ã€‘æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†ï¼ˆç»ˆæä¼˜åŒ–ç‰ˆï¼‰...")

ultimate_stopwords = {
    'vulnerability', 'vulnerable', 'user', 'users', 'attacker', 'attackers',
    'attack', 'allows', 'allow', 'via', 'issue', 'issues', 'affected',
    'affect', 'affects', 'versions', 'version', 'prior', 'before', 'after',
    'could', 'can', 'may', 'might', 'cause', 'caused', 'lead', 'leads',
    'result', 'results', 'using', 'use', 'used', 'remote', 'local',
    'unspecified', 'unknown', 'certain', 'multiple', 'arbitrary',
    'sensitive', 'malicious', 'crafted', 'specially', 'specially crafted',
    'file', 'files', 'code', 'access', 'site', 'sites', 'system', 'systems',
    'web', 'application', 'applications', 'server', 'service', 'services',
    'data', 'information', 'resource', 'resources', 'component', 'module',
    'function', 'functions', 'feature', 'features', 'parameter', 'parameters',
    'input', 'inputs', 'output', 'request', 'response', 'http', 'https',
    'network', 'networks', 'protocol', 'interface', 'default', 'configuration',
    'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her',
    'was', 'one', 'our', 'out', 'has', 'him', 'his', 'how', 'its', 'may',
    'new', 'now', 'old', 'see', 'two', 'who', 'did', 'she', 'use', 'way',
    'many', 'any', 'say', 'man', 'try', 'ask', 'end', 'why', 'let', 'put',
    'tell', 'very', 'when', 'come', 'here', 'look', 'make', 'seem', 'over',
    'such', 'take', 'than', 'them', 'well', 'were', 'your', 'about', 'could',
    'other', 'after', 'first', 'never', 'these', 'think', 'where', 'being',
    'every', 'great', 'might', 'shall', 'still', 'while', 'this', 'that',
    'with', 'have', 'from', 'they', 'know', 'want', 'been', 'good', 'much',
    'some', 'time', 'would', 'there', 'which', 'their', 'what', 'said', 'each',
    'will', 'also', 'does', 'did', 'through', 'should', 'product', 'products',
    'vendor', 'vendors', 'developer', 'developers'
}


def preprocess_text_v3(text):
    if pd.isna(text):
        return ""

    text = re.sub(r'cross[\s-]?site[\s-]?scripting', 'xss_attack', text, flags=re.IGNORECASE)
    text = re.sub(r'cross[\s-]?site', 'xss_attack', text, flags=re.IGNORECASE)
    text = re.sub(r'sql[\s-]?injection', 'sql_injection', text, flags=re.IGNORECASE)
    text = re.sub(r'sql[\s-]?command', 'sql_injection', text, flags=re.IGNORECASE)
    text = re.sub(r'path[\s-]?traversal|directory[\s-]?traversal', 'path_traversal', text, flags=re.IGNORECASE)
    text = re.sub(r'remote[\s-]?code[\s-]?execution', 'rce', text, flags=re.IGNORECASE)
    text = re.sub(r'command[\s-]?injection', 'command_injection', text, flags=re.IGNORECASE)
    text = re.sub(r'file[\s-]?upload', 'file_upload', text, flags=re.IGNORECASE)
    text = re.sub(r'file[\s-]?inclusion', 'file_inclusion', text, flags=re.IGNORECASE)
    text = re.sub(r'buffer[\s-]?overflow', 'buffer_overflow', text, flags=re.IGNORECASE)
    text = re.sub(r'privilege[\s-]?escalation', 'privilege_escalation', text, flags=re.IGNORECASE)
    text = re.sub(r'authentication[\s-]?bypass', 'auth_bypass', text, flags=re.IGNORECASE)

    text = re.sub(r'CVE-\d{4}-\d{4,}', ' ', text)
    text = re.sub(r'\d+\.\d+(\.\d+)*', ' ', text)
    text = re.sub(r'http[s]?://\S+', ' ', text)
    text = re.sub(r'\S+@\S+', ' ', text)

    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()

    words = [w for w in text.split()
             if len(w) > 2
             and w not in ultimate_stopwords
             and not w.isdigit()]

    return ' '.join(words)


web_df['processed'] = web_df['Description'].apply(preprocess_text_v3)

valid_mask = web_df['processed'].str.len() > 0
web_df = web_df[valid_mask].copy()

print(f"âœ… é¢„å¤„ç†å®Œæˆï¼š{len(web_df)} æ¡æœ‰æ•ˆè®°å½•")

web_df[['Name', 'Description', 'processed', 'Year']].to_csv('step3_processed_2021_2025.csv', index=False)
print("é¢„å¤„ç†ç»“æœå·²ä¿å­˜è‡³ï¼šstep3_processed_2021_2025.csv")

# =============================================================================
# ç¬¬å››æ­¥ï¼šæè¿°æ€§ç»Ÿè®¡
# =============================================================================
print("\n" + "=" * 60)
print("ã€æ­¥éª¤4ã€‘æè¿°æ€§ç»Ÿè®¡...")

web_df['text_length'] = web_df['Description'].str.len()
web_df['word_count'] = web_df['processed'].str.split().str.len()

print(f"æ ·æœ¬é‡ï¼š{len(web_df)}")
print(f"å¹³å‡é•¿åº¦ï¼š{web_df['text_length'].mean():.1f} å­—ç¬¦")
print(f"å¹³å‡è¯æ•°ï¼š{web_df['word_count'].mean():.1f} è¯")

stats_df = pd.DataFrame({
    'æŒ‡æ ‡': ['æ ·æœ¬é‡', 'å¹³å‡é•¿åº¦(å­—ç¬¦)', 'ä¸­ä½æ•°é•¿åº¦(å­—ç¬¦)',
             'æœ€çŸ­', 'æœ€é•¿', 'å¹³å‡è¯æ•°', 'ä¸­ä½æ•°è¯æ•°'],
    'æ•°å€¼': [
        len(web_df),
        round(web_df['text_length'].mean(), 1),
        round(web_df['text_length'].median(), 1),
        web_df['text_length'].min(),
        web_df['text_length'].max(),
        round(web_df['word_count'].mean(), 1),
        round(web_df['word_count'].median(), 1)
    ]
})
stats_df.to_csv('step4_statistics_2021_2025.csv', index=False)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].hist(web_df['text_length'], bins=30, color='skyblue', edgecolor='black')
axes[0].set_title('Text Length Distribution')
axes[0].set_xlabel('Character Count')
axes[0].set_ylabel('Frequency')

axes[1].hist(web_df['word_count'], bins=30, color='lightgreen', edgecolor='black')
axes[1].set_title('Word Count Distribution')
axes[1].set_xlabel('Word Count')

plt.tight_layout()
plt.savefig('step4_distribution_2021_2025.png', dpi=300)
print("å›¾è¡¨å·²ä¿å­˜è‡³ï¼šstep4_distribution_2021_2025.png")

# =============================================================================
# ç¬¬äº”æ­¥ï¼šTF-IDFå…³é”®è¯æå–ï¼ˆæ•´ä½“äº”å¹´ï¼‰
# =============================================================================
print("\n" + "=" * 60)
print("ã€æ­¥éª¤5ã€‘TF-IDFå…³é”®è¯æå–ï¼ˆæ•´ä½“äº”å¹´ï¼‰...")

n_samples = len(web_df)
print(f"ä½¿ç”¨ {n_samples} æ¡è®°å½•è¿›è¡Œåˆ†æ")

vectorizer = TfidfVectorizer(
    max_features=100,
    ngram_range=(2, 2),
    min_df=3 if n_samples > 30 else 2,
    max_df=0.5,
    stop_words=None,
    token_pattern=r'\b[a-zA-Z_]+\b'
)

try:
    tfidf_matrix = vectorizer.fit_transform(web_df['processed'])
    feature_names = vectorizer.get_feature_names_out()
    if len(feature_names) < 10:
        raise ValueError("ç‰¹å¾å¤ªå°‘")
except Exception as e:
    print(f"ä¸¥æ ¼æ¨¡å¼ç‰¹å¾ä¸è¶³ï¼Œåˆ‡æ¢åˆ°å®½æ¾æ¨¡å¼...")
    vectorizer = TfidfVectorizer(
        max_features=100,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.6,
        stop_words=None
    )
    tfidf_matrix = vectorizer.fit_transform(web_df['processed'])
    feature_names = vectorizer.get_feature_names_out()

print(f"æå–åˆ° {len(feature_names)} ä¸ªç‰¹å¾è¯")

mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)

top_n = min(30, len(feature_names))
top_indices = mean_scores.argsort()[-top_n:][::-1]
top_keywords = [(feature_names[i], mean_scores[i]) for i in top_indices]

print(f"\nTop-{top_n} å…³é”®è¯ï¼ˆæ”»å‡»æŠ€æœ¯ focusedï¼‰ï¼š")
print("-" * 50)
for i, (word, score) in enumerate(top_keywords, 1):
    print(f"{i:2d}. {word:30s} {score:.4f}")

keywords_df = pd.DataFrame(top_keywords, columns=['Keyword', 'TF-IDF_Score'])
keywords_df.to_csv('step5_keywords_2021_2025_optimized.csv', index=False)

plt.figure(figsize=(12, 10))
words = [w for w, _ in top_keywords]
scores = [s for _, s in top_keywords]
plt.barh(range(len(words)), scores, color='steelblue')
plt.yticks(range(len(words)), words)
plt.xlabel('TF-IDF Score')
plt.title(f'Top-{top_n} Attack Technique Keywords (2021-2025)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('step5_keywords_2021_2025_optimized.png', dpi=300)
print("å›¾è¡¨å·²ä¿å­˜è‡³ï¼šstep5_keywords_2021_2025_optimized.png")

# =============================================================================
# ç¬¬å…­æ­¥ï¼šå…³é”®è¯åˆ†ç±»åˆ†æ
# =============================================================================
print("\n" + "=" * 60)
print("ã€æ­¥éª¤6ã€‘å…³é”®è¯åˆ†ç±»åˆ†æ...")

category_rules = {
    'æ”»å‡»æŠ€æœ¯': ['injection', 'overflow', 'bypass', 'execution', 'xss', 'sql',
                 'csrf', 'traversal', 'disclosure', 'denial', 'authentication', 'command'],
    'å—å½±å“ç»„ä»¶': ['web', 'server', 'php', 'apache', 'nginx', 'tomcat',
                   'wordpress', 'drupal', 'cms', 'database', 'oracle', 'mysql', 'postgres'],
    'æ”»å‡»åæœ': ['information', 'arbitrary', 'unauthorized', 'privilege', 'access', 'remote'],
    'å…¶ä»–': []
}


def classify_keyword(keyword):
    for category, keywords in category_rules.items():
        if any(kw in keyword for kw in keywords):
            return category
    return 'å…¶ä»–'


keywords_df['Category'] = keywords_df['Keyword'].apply(classify_keyword)

category_stats = keywords_df.groupby('Category').agg({
    'Keyword': 'count',
    'TF-IDF_Score': 'sum'
}).reset_index()
category_stats.columns = ['Category', 'Count', 'Total_Score']
category_stats['Percentage'] = category_stats['Count'] / len(keywords_df) * 100

print("\nå…³é”®è¯åˆ†ç±»ç»Ÿè®¡ï¼š")
print("-" * 50)
print(category_stats.to_string(index=False))

keywords_df.to_csv('step6_categorized_2021_2025.csv', index=False)

fig, ax = plt.subplots(figsize=(8, 8))
ax.pie(category_stats['Count'], labels=category_stats['Category'],
       autopct='%1.1f%%', startangle=90)
ax.set_title('Keywords by Category (2021-2025)')
plt.savefig('step6_pie_2021_2025.png', dpi=300)
print("åˆ†ç±»é¥¼å›¾å·²ä¿å­˜è‡³ï¼šstep6_pie_2021_2025.png")

# =============================================================================
# ç¬¬ä¸ƒæ­¥ï¼šå•ä¸ªæ–‡æ¡£å…³é”®è¯åˆ†æï¼ˆç¤ºä¾‹ï¼‰
# =============================================================================
print("\n" + "=" * 60)
print("ã€æ­¥éª¤7ã€‘å•ä¸ªæ–‡æ¡£å…³é”®è¯åˆ†æ...")


def get_doc_keywords(doc_idx, n=10):
    row = tfidf_matrix[doc_idx].toarray()[0]
    top_indices = row.argsort()[-n:][::-1]
    return [(feature_names[i], row[i]) for i in top_indices if row[i] > 0]


print("\nå•ä¸ªæ–‡æ¡£å…³é”®è¯ç¤ºä¾‹ï¼š")
for i in range(min(3, len(web_df))):
    print(f"\nã€æ–‡æ¡£ {i + 1}ã€‘: {web_df.iloc[i]['Name']}")
    print(f"åŸå§‹æè¿°: {web_df.iloc[i]['Description'][:100]}...")
    doc_keywords = get_doc_keywords(i, n=5)
    print("Top-5 å…³é”®è¯:")
    for word, score in doc_keywords:
        print(f"  - {word}: {score:.4f}")

# =============================================================================
# ç¬¬å…«æ­¥ï¼šè·¨å¹´è¶‹åŠ¿åˆ†æï¼ˆæ–°å¢ï¼‰
# =============================================================================
print("\n" + "=" * 60)
print("ã€æ­¥éª¤8ã€‘è·¨å¹´è¶‹åŠ¿åˆ†æ...")

# è·å–å®é™…å­˜åœ¨çš„å¹´ä»½ï¼ˆæ’é™¤unknownï¼Œå¹¶ç¡®ä¿ä¸º2021-2025ï¼‰
years = sorted([y for y in web_df['Year'].unique() if y.isdigit() and 2021 <= int(y) <= 2025])
print(f"åˆ†æå¹´ä»½: {years}")

# å­˜å‚¨æ¯å¹´çš„Topå…³é”®è¯
yearly_keywords = {}
yearly_top10 = {}

for year in years:
    print(f"\n--- åˆ†æ {year} å¹´æ•°æ® ---")
    year_df = web_df[web_df['Year'] == year].copy()
    if len(year_df) < 5:
        print(f"{year} å¹´æ•°æ®ä¸è¶³ï¼ˆ{len(year_df)}æ¡ï¼‰ï¼Œè·³è¿‡")
        continue

    # å¯¹å½“å¹´æ•°æ®é‡æ–°è®­ç»ƒTF-IDFï¼ˆä½¿ç”¨ç›¸åŒçš„å‚æ•°ï¼‰
    vectorizer_year = TfidfVectorizer(
        max_features=50,
        ngram_range=(2, 2),
        min_df=2,
        max_df=0.5,
        stop_words=None,
        token_pattern=r'\b[a-zA-Z_]+\b'
    )
    try:
        tfidf_year = vectorizer_year.fit_transform(year_df['processed'])
        feature_names_year = vectorizer_year.get_feature_names_out()
        if len(feature_names_year) < 5:
            raise ValueError("ç‰¹å¾å¤ªå°‘")
    except Exception as e:
        print(f"ä¸¥æ ¼æ¨¡å¼ç‰¹å¾ä¸è¶³ï¼Œåˆ‡æ¢åˆ°å®½æ¾æ¨¡å¼...")
        vectorizer_year = TfidfVectorizer(
            max_features=50,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.6,
            stop_words=None
        )
        tfidf_year = vectorizer_year.fit_transform(year_df['processed'])
        feature_names_year = vectorizer_year.get_feature_names_out()

    mean_scores_year = np.mean(tfidf_year.toarray(), axis=0)

    # å–Top-20ï¼ˆå¯è°ƒæ•´ï¼‰
    top_n_year = min(20, len(feature_names_year))
    top_indices_year = mean_scores_year.argsort()[-top_n_year:][::-1]
    top_keywords_year = [(feature_names_year[i], mean_scores_year[i]) for i in top_indices_year]

    yearly_keywords[year] = top_keywords_year
    yearly_top10[year] = top_keywords_year[:10]  # åªä¿ç•™å‰10ç”¨äºæŠ¥å‘Š

    # ä¿å­˜æ¯å¹´å…³é”®è¯
    pd.DataFrame(top_keywords_year, columns=['Keyword', 'TF-IDF_Score']).to_csv(f'step8_keywords_{year}.csv', index=False)
    print(f"{year}å¹´Top-10:")
    for i, (kw, sc) in enumerate(top_keywords_year[:10], 1):
        print(f"  {i:2d}. {kw:30s} {sc:.4f}")

# ç”Ÿæˆè¶‹åŠ¿å¯è§†åŒ–ï¼ˆæŠ˜çº¿å›¾+çƒ­åŠ›å›¾ï¼‰
if len(yearly_keywords) >= 2:
    # æ”¶é›†æ‰€æœ‰å…³é”®è¯
    all_keywords = set()
    for year, kw_list in yearly_keywords.items():
        for kw, _ in kw_list:
            all_keywords.add(kw)

    # æ„å»ºæ•°æ®æ¡†ï¼Œè¡Œ=å…³é”®è¯ï¼Œåˆ—=å¹´ä»½ï¼Œå€¼=TF-IDFå¾—åˆ†
    trend_data = []
    for kw in all_keywords:
        row = {'Keyword': kw}
        for year in yearly_keywords:
            score = 0.0
            for yk, ys in yearly_keywords[year]:
                if yk == kw:
                    score = ys
                    break
            row[year] = score
        trend_data.append(row)

    trend_df = pd.DataFrame(trend_data).set_index('Keyword')
    # ç­›é€‰åœ¨è‡³å°‘2å¹´å‡ºç°çš„å…³é”®è¯
    trend_df = trend_df[(trend_df > 0).sum(axis=1) >= 2]
    trend_df = trend_df.sort_index(axis=1)  # æŒ‰å¹´ä»½æ’åº

    # ä¿å­˜è¶‹åŠ¿æ•°æ®
    trend_df.to_csv('step8_trend_data.csv')

    # æŠ˜çº¿å›¾
    plt.figure(figsize=(12, 6))
    for kw in trend_df.index:
        plt.plot(trend_df.columns.astype(str), trend_df.loc[kw], marker='o', label=kw)
    plt.xlabel('Year')
    plt.ylabel('TF-IDF Score')
    plt.title('Keyword Trend Analysis (2021-2025)')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig('step8_trend_lines_2021_2025.png', dpi=300)
    print("è¶‹åŠ¿æŠ˜çº¿å›¾å·²ä¿å­˜è‡³ï¼šstep8_trend_lines_2021_2025.png")

    # çƒ­åŠ›å›¾ï¼ˆå¦‚æœseabornå¯ç”¨ï¼‰
    if HAS_SEABORN:
        plt.figure(figsize=(10, max(6, len(trend_df)*0.3)))
        sns.heatmap(trend_df, annot=True, fmt='.3f', cmap='YlOrRd',
                    cbar_kws={'label': 'TF-IDF Score'})
        plt.title('Keyword TF-IDF Heatmap by Year')
        plt.tight_layout()
        plt.savefig('step8_trend_heatmap_2021_2025.png', dpi=300)
        print("çƒ­åŠ›å›¾å·²ä¿å­˜è‡³ï¼šstep8_trend_heatmap_2021_2025.png")
    else:
        print("seabornæœªå®‰è£…ï¼Œè·³è¿‡çƒ­åŠ›å›¾")

# =============================================================================
# ç¬¬ä¹æ­¥ï¼šç”Ÿæˆå®Œæ•´æŠ¥å‘Šï¼ˆåŸç¬¬å…«æ­¥ï¼‰
# =============================================================================
print("\n" + "=" * 60)
print("ã€æ­¥éª¤9ã€‘ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

# æ„å»ºæŠ¥å‘Šå†…å®¹
report = f"""
å¨èƒæƒ…æŠ¥å…³é”®è¯æå–åˆ†ææŠ¥å‘Šï¼ˆ2021-2025è¿‘äº”å¹´ï¼‰
{'=' * 60}

ä¸€ã€æ•°æ®æ¦‚å†µ
- åˆ†æå¯¹è±¡ï¼šCVEæ¼æ´æè¿°æ–‡æœ¬ï¼ˆ2021-2025å¹´ï¼‰
- ç­›é€‰æ¡ä»¶ï¼šWebåº”ç”¨ç›¸å…³æ¼æ´
- æ ·æœ¬æ•°é‡ï¼š{len(web_df)} æ¡
- å¹³å‡é•¿åº¦ï¼š{web_df['text_length'].mean():.1f} å­—ç¬¦
- å¹³å‡è¯æ•°ï¼š{web_df['word_count'].mean():.1f} è¯

äºŒã€æ•´ä½“TF-IDFåˆ†æç»“æœ
- ç‰¹å¾è¯æ•°é‡ï¼š{len(feature_names)} ä¸ª
- æå–å…³é”®è¯ï¼šTop-{top_n}

ä¸‰ã€å…³é”®è¯åˆ†ç±»ç»Ÿè®¡
{category_stats.to_string(index=False)}

å››ã€Top-10 å…³é”®è¯ï¼ˆæ•´ä½“äº”å¹´ï¼‰
"""
for i, (word, score) in enumerate(top_keywords[:10], 1):
    report += f"{i:2d}. {word} (TF-IDF: {score:.4f})\n"

# ===== æ–°å¢ï¼šè·¨å¹´è¶‹åŠ¿æ‘˜è¦ =====
if yearly_top10:
    report += "\näº”ã€è·¨å¹´è¶‹åŠ¿æ‘˜è¦ï¼ˆæ¯å¹´Top-3å…³é”®è¯ï¼‰\n"
    for year in sorted(yearly_top10.keys()):
        report += f"{year}å¹´: "
        top3 = yearly_top10[year][:3]
        terms = [f"{kw}({score:.3f})" for kw, score in top3]
        report += ", ".join(terms) + "\n"

report += f"""
å…­ã€æ•°æ®æ–‡ä»¶æ¸…å•ï¼ˆ2021-2025ï¼‰
- step2_web_cves_2021_2025.csv: ç­›é€‰åçš„Webç›¸å…³CVE ({len(web_df)}æ¡)
- step3_processed_2021_2025.csv: é¢„å¤„ç†åçš„æ•°æ®
- step4_statistics_2021_2025.csv: æè¿°æ€§ç»Ÿè®¡
- step4_distribution_2021_2025.png: æ–‡æœ¬é•¿åº¦åˆ†å¸ƒå›¾
- step5_keywords_2021_2025.csv: TF-IDFå…³é”®è¯ç»“æœ (Top-{top_n})
- step5_keywords_2021_2025.png: å…³é”®è¯æ¡å½¢å›¾
- step6_categorized_2021_2025.csv: åˆ†ç±»åçš„å…³é”®è¯
- step6_pie_2021_2025.png: åˆ†ç±»é¥¼å›¾
- step8_keywords_YYYY.csv: å„å¹´ä»½å•ç‹¬çš„å…³é”®è¯æ–‡ä»¶ï¼ˆYYYYä¸ºå¹´ä»½ï¼‰
- step8_trend_data.csv: è¶‹åŠ¿åˆ†ææ•°æ®è¡¨
- step8_trend_lines_2021_2025.png: å…³é”®è¯è¶‹åŠ¿æŠ˜çº¿å›¾
- step8_trend_heatmap_2021_2025.png: å…³é”®è¯çƒ­åŠ›å›¾ï¼ˆè‹¥seabornå¯ç”¨ï¼‰
- analysis_report_2021_2025.txt: æœ¬æŠ¥å‘Š
"""

with open('analysis_report_2021_2025.txt', 'w', encoding='utf-8') as f:
    f.write(report)

print("åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³ï¼šanalysis_report_2021_2025.txt")
print("\n" + "=" * 60)
print("âœ… åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœæ–‡ä»¶ï¼ˆ2021-2025ï¼‰ï¼š")
print("  - step2_web_cves_2021_2025.csv")
print("  - step3_processed_2021_2025.csv")
print("  - step4_statistics_2021_2025.csv")
print("  - step4_distribution_2021_2025.png")
print("  - step5_keywords_2021_2025.csv")
print("  - step5_keywords_2021_2025.png")
print("  - step6_categorized_2021_2025.csv")
print("  - step6_pie_2021_2025.png")
print("  - step8_keywords_YYYY.csv (å„å¹´ä»½)")
print("  - step8_trend_data.csv")
print("  - step8_trend_lines_2021_2025.png")
if HAS_SEABORN:
    print("  - step8_trend_heatmap_2021_2025.png")
print("  - analysis_report_2021_2025.txt")
print("=" * 60)
